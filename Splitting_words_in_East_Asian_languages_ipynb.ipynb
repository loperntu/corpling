{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loperntu/corpling/blob/master/Splitting_words_in_East_Asian_languages_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezXnqehPCIDg"
      },
      "source": [
        "# Word-splitting and text segmentation in East Asian languages\n",
        "\n",
        "As different as they are, Chinese, Japanese and Korean are lumped together as [CJK languages](https://en.wikipedia.org/wiki/CJK_characters) when discussed from an English-language point of view. One reason they're considered similar is that **spacing** is not used in the same way as in English. While analyzing English requires splitting sentences based on spaces, one difficulty for this language set (and others) is **determining where the breaks between words are.** In this section we will review how to use libraries to segment words in these languages as well as Thai and Vietnamese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJk65qlCIDj"
      },
      "source": [
        "<p class=\"reading-options\">\n",
        "  <a class=\"btn\" href=\"/text-analysis/splitting-words-in-east-asian-languages\">\n",
        "    <i class=\"fa fa-sm fa-book\"></i>\n",
        "    Read online\n",
        "  </a>\n",
        "  <a class=\"btn\" href=\"/text-analysis/notebooks/Splitting words in East Asian languages.ipynb\">\n",
        "    <i class=\"fa fa-sm fa-download\"></i>\n",
        "    Download notebook\n",
        "  </a>\n",
        "  <a class=\"btn\" href=\"https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/Splitting words in East Asian languages.ipynb\" target=\"_new\">\n",
        "    <i class=\"fa fa-sm fa-laptop\"></i>\n",
        "    Interactive version\n",
        "  </a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmUAVt5GCIDj"
      },
      "source": [
        "## Using libraries\n",
        "\n",
        "**Segmenting** is the process of splitting a text into separate words. There isn't always a \"right\" answer as to what the split should be, so you might have to **try a few different libraries** before you feel a good fit. The recommendations below aren't necessarily the best Python packages, they're just ones that had a bit of activity, seemingly-decent interfaces/documentation, and no external installs.\n",
        "\n",
        "### Using these libraries with scikit-learn\n",
        "\n",
        "After reading this page, you might want to learn how to use these libraries with scikit-learn vectorizers. In that case, check out tutorial on [how to make scikit-learn vectorizers work with Japanese, Chinese, and other East Asian languages page](/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese/) after you read this page.\n",
        "\n",
        "## Chinese: jieba\n",
        "\n",
        "The Chinese word segmentation library [jieba](https://github.com/fxsjy/jieba) is very popular when analyzing Chinese text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G73evaWCIDk"
      },
      "outputs": [],
      "source": [
        "#!pip install jieba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJI5n7YaCIDl"
      },
      "source": [
        "Jieba has a few different techniques we can use to segment words. We can read the documentation to get into details, but one major question is whether we want the **smallest possible divisions.**\n",
        "\n",
        "Using `lcut` gives us individual words and what might be considered _noun phrases_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oAX2WvCCIDl",
        "outputId": "3f5a8040-e89d-453a-e9c0-cc38403c281c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['我', '来到', '北京', '清华大学']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import jieba\n",
        "\n",
        "jieba.lcut('我来到北京清华大学')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aZo_Jt0CIDm"
      },
      "source": [
        "If we want to divide things up a bit more, we can add `cut_all=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssoxAHJDCIDn",
        "outputId": "aceedfc0-1edf-4d9a-f51d-edcfa622174f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['我', '来到', '北京', '清华', '清华大学', '华大', '大学']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jieba.lcut('我来到北京清华大学', cut_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-Qkb5etCIDn"
      },
      "source": [
        "The big difference is `cut_all` will split something like `清华大学` into both `清华` and `华大`.\n",
        "\n",
        "When might we use one compared to the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVYqAViRCIDo"
      },
      "source": [
        "## Japanese: nagisa\n",
        "\n",
        "For Japanese, you'll be using the library [nagisa](https://github.com/taishi-i/nagisa)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMRpdZbKCIDo"
      },
      "outputs": [],
      "source": [
        "#!pip install nagisa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy833yasCIDo",
        "outputId": "39864d85-d0b9-4f74-a60b-e06448ba7dec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Python', 'で', '簡単', 'に', '使える', 'ツール', 'です']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nagisa\n",
        "\n",
        "text = 'Pythonで簡単に使えるツールです'\n",
        "doc = nagisa.tagging(text)\n",
        "\n",
        "doc.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KNfL-rtCIDo"
      },
      "source": [
        "In addition to simple tokenization, nagisa will also do **part-of-speech tagging**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao-F0SNuCIDp",
        "outputId": "c5a9c5f4-a667-41fe-8ca0-44f3e2c51016"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['名詞', '助詞', '形状詞', '助動詞', '動詞', '名詞', '助動詞']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words.postags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6vYr31zCIDp"
      },
      "source": [
        "This allows you to do things like pluck out all of the nouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAiD5YFVCIDp",
        "outputId": "8d6a7d8f-f069-4517-82e6-9f1585ed3dfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Python', 'ツール']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nouns = nagisa.extract(text, extract_postags=['名詞'])\n",
        "nouns.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ6XPtmdCIDp"
      },
      "source": [
        "## Korean: KoNLPy\n",
        "\n",
        "Korean does use spaces, but certain characters combine with the \"actual\" words, so you can't just split on spaces. [KoNPLy](http://konlpy.org/) has several engines that will help you with this. [See a comparison chart of the different engines here](http://konlpy.org/en/latest/morph/#performance-analysis) or [find more specific details](https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH7EF0PrCIDq"
      },
      "outputs": [],
      "source": [
        "#!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAUTTN3CCIDq"
      },
      "outputs": [],
      "source": [
        "phrase = \"아버지가방에들어가신다\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kskkd9AqCIDq",
        "outputId": "a4113677-36a9-405e-f6bf-1860b828db1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['아버지가방에들어가', '이', '시ㄴ다']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "hannanum.morphs(phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StBwO89RCIDq",
        "outputId": "67813bec-fb96-46c7-cfc2-bc1642af6027"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma.morphs(phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3AA9mBCCIDq",
        "outputId": "e191ff50-c6a9-422f-c9b1-edb164659db3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "komoran.morphs(phrase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAzLWum3CIDr"
      },
      "source": [
        "## Thai: tltk\n",
        "\n",
        "You can find [many Thai NLP packages here](https://github.com/kobkrit/nlp_thai_resources), but we'll focus on [tltk](https://pypi.org/project/tltk/). It doesn't have the best documentation and it might not be the most accurate, but it **doesn't require us to install anything extra** (e.g. Tensorflow) and that's the absolutely only reason why we're using it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ0XHN6TCIDr"
      },
      "outputs": [],
      "source": [
        "#!pip install tltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y276yQUzCIDr",
        "outputId": "e9bf8092-4463-4298-804f-87ca031c3e1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('สำนักงาน', 'NOUN'),\n",
              "  ('เขต', 'NOUN'),\n",
              "  ('จตุจักร', 'PROPN'),\n",
              "  ('ชี้แจง', 'VERB'),\n",
              "  ('ว่า', 'SCONJ'),\n",
              "  ('<s/>', 'PUNCT')],\n",
              " [('ได้', 'AUX'),\n",
              "  ('นำ', 'VERB'),\n",
              "  ('ป้ายประกาศ', 'NOUN'),\n",
              "  ('เตือน', 'VERB'),\n",
              "  ('ปลิง', 'NOUN'),\n",
              "  ('ไป', 'VERB'),\n",
              "  ('ปัก', 'VERB'),\n",
              "  ('ตาม', 'ADP'),\n",
              "  ('แหล่งน้ำ', 'NOUN'),\n",
              "  (' \\n', 'NOUN'),\n",
              "  ('ใน', 'ADP'),\n",
              "  ('เขต', 'NOUN'),\n",
              "  ('อำเภอ', 'NOUN'),\n",
              "  ('เมือง', 'NOUN'),\n",
              "  ('<s/>', 'PUNCT')],\n",
              " [('จังหวัด', 'NOUN'),\n",
              "  ('อ่างทอง', 'PROPN'),\n",
              "  ('<s/>', 'PUNCT'),\n",
              "  ('หลังจาก', 'SCONJ'),\n",
              "  ('นาย', 'NOUN'),\n",
              "  ('สุ', 'PROPN'),\n",
              "  ('กิจ', 'NOUN'),\n",
              "  ('<s/>', 'PUNCT')],\n",
              " [('อายุ', 'NOUN'), ('<s/>', 'PUNCT')],\n",
              " [('65 ', 'NUM'), ('ปี', 'NOUN'), ('<s/>', 'PUNCT')],\n",
              " [('ถูก', 'AUX'),\n",
              "  ('ปลิง', 'VERB'),\n",
              "  ('กัด', 'VERB'),\n",
              "  ('แล้ว', 'ADV'),\n",
              "  ('ไม่ได้', 'AUX'),\n",
              "  ('ไป', 'VERB'),\n",
              "  ('พบ', 'VERB'),\n",
              "  ('แพทย์', 'NOUN'),\n",
              "  ('<s/>', 'PUNCT')]]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tltk\n",
        "\n",
        "phrase = \"\"\"สำนักงานเขตจตุจักรชี้แจงว่า ได้นำป้ายประกาศเตือนปลิงไปปักตามแหล่งน้ำ \n",
        "ในเขตอำเภอเมือง จังหวัดอ่างทอง หลังจากนายสุกิจ อายุ 65 ปี ถูกปลิงกัดแล้วไม่ได้ไปพบแพทย์\"\"\"\n",
        "\n",
        "pieces = tltk.nlp.pos_tag(phrase)\n",
        "pieces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGzf5tTBCIDr"
      },
      "source": [
        "If you just want to split out everything individually, you'll need to jump through a tiny hoop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBc14pFGCIDr",
        "outputId": "b4f087ad-fb71-461c-835f-45bfbfa30f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('สำนักงาน', 'NOUN'), ('เขต', 'NOUN'), ('จตุจักร', 'PROPN'), ('ชี้แจง', 'VERB'), ('ว่า', 'SCONJ'), ('<s/>', 'PUNCT'), ('ได้', 'AUX'), ('นำ', 'VERB'), ('ป้ายประกาศ', 'NOUN'), ('เตือน', 'VERB'), ('ปลิง', 'NOUN'), ('ไป', 'VERB'), ('ปัก', 'VERB'), ('ตาม', 'ADP'), ('แหล่งน้ำ', 'NOUN'), (' \\n', 'NOUN'), ('ใน', 'ADP'), ('เขต', 'NOUN'), ('อำเภอ', 'NOUN'), ('เมือง', 'NOUN'), ('<s/>', 'PUNCT'), ('จังหวัด', 'NOUN'), ('อ่างทอง', 'PROPN'), ('<s/>', 'PUNCT'), ('หลังจาก', 'SCONJ'), ('นาย', 'NOUN'), ('สุ', 'PROPN'), ('กิจ', 'NOUN'), ('<s/>', 'PUNCT'), ('อายุ', 'NOUN'), ('<s/>', 'PUNCT'), ('65 ', 'NUM'), ('ปี', 'NOUN'), ('<s/>', 'PUNCT'), ('ถูก', 'AUX'), ('ปลิง', 'VERB'), ('กัด', 'VERB'), ('แล้ว', 'ADV'), ('ไม่ได้', 'AUX'), ('ไป', 'VERB'), ('พบ', 'VERB'), ('แพทย์', 'NOUN'), ('<s/>', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "words = [word for piece in pieces for word in piece]\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnj-fFORCIDs"
      },
      "source": [
        "If you'd like to cast away the part of speech, just ask for the first part of the pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fli_pObACIDs",
        "outputId": "578ad73f-6780-48bf-b357-1043dc7914d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['สำนักงาน', 'เขต', 'จตุจักร', 'ชี้แจง', 'ว่า', '<s/>', 'ได้', 'นำ', 'ป้ายประกาศ', 'เตือน', 'ปลิง', 'ไป', 'ปัก', 'ตาม', 'แหล่งน้ำ', ' \\n', 'ใน', 'เขต', 'อำเภอ', 'เมือง', '<s/>', 'จังหวัด', 'อ่างทอง', '<s/>', 'หลังจาก', 'นาย', 'สุ', 'กิจ', '<s/>', 'อายุ', '<s/>', '65 ', 'ปี', '<s/>', 'ถูก', 'ปลิง', 'กัด', 'แล้ว', 'ไม่ได้', 'ไป', 'พบ', 'แพทย์', '<s/>']\n"
          ]
        }
      ],
      "source": [
        "words = [word[0] for piece in pieces for word in piece]\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAtY66KFCIDs"
      },
      "source": [
        "## Vietnamese: pyvi\n",
        "\n",
        "For Vietnamese we'll use [pyvi](https://github.com/trungtv/pyvi). There are [plenty of other options](https://github.com/undertheseanlp/NLP-Vietnamese-progress/blob/master/tasks/word_segmentation.md), but the best ones all involve installing Java and separate packages. We'll use pyvi to keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhyx6YX0CIDs"
      },
      "outputs": [],
      "source": [
        "#!pip install pyvi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUqvAALYCIDs"
      },
      "source": [
        "Weirdly, when you run the `tokenize` method you get a string back..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3EW5swjCIDs",
        "outputId": "c1970a48-4f1f-453c-9642-e286ca35921f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Trường đại_học bách_khoa hà_nội'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "\n",
        "words = ViTokenizer.tokenize(u\"Trường đại học bách khoa hà nội\")\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JvJwUV3CIDs",
        "outputId": "6977e392-2588-46da-a81c-49ff9b26f1e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Trường', 'đại_học', 'bách_khoa', 'hà_nội']"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = words.split(\" \")\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlMrdVHWCIDt"
      },
      "source": [
        "But! If you're also hunting for parts of speech, you end up with a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo209qzECIDt",
        "outputId": "965240c7-52bd-4580-f746-5a9528036dd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Trường', 'đại_học', 'Bách_Khoa', 'Hà_Nội'], ['N', 'N', 'Np', 'Np'])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ViPosTagger.postagging(ViTokenizer.tokenize(u\"Trường đại học Bách Khoa Hà Nội\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhVQjIixCIDt"
      },
      "source": [
        "You can split the words and parts of speech apart easily enough, if you need them in separate variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fneP00kCIDt",
        "outputId": "1355c08d-6967-4eb3-fd06-aba5a37ab8b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "words are ['Trường', 'đại_học', 'Bách_Khoa', 'Hà_Nội']\n",
            "pos are ['N', 'N', 'Np', 'Np']\n"
          ]
        }
      ],
      "source": [
        "words, pos = ViPosTagger.postagging(ViTokenizer.tokenize(u\"Trường đại học Bách Khoa Hà Nội\"))\n",
        "print('words are', words)\n",
        "print('pos are', pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aApJFlu2CIDt"
      },
      "source": [
        "If you'd like them matched up (like in some of the examples above), you can use `zip` to pair the word and the part of speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln7cU5DZCIDt",
        "outputId": "7637eece-fd43-403d-c7ce-cc14d3bf7846"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Trường', 'N'), ('đại_học', 'N'), ('Bách_Khoa', 'Np'), ('Hà_Nội', 'Np')]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(zip(words, pos))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xBQ1XwgCIDt"
      },
      "source": [
        "## Review\n",
        "\n",
        "In this section we looked at **tokenizing** text in several different languages that can't just be split with spaces. To discover how to use these libraries with scikit-learn vectorizers, check out tutorial on [how to make scikit-learn vectorizers work with Japanese, Chinese, and other East Asian languages page](/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgAcf_z6CIDt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}